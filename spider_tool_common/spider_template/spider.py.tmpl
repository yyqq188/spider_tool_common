# -*- coding: utf-8 -*-
import scrapy
#from scrapys.scrapys.items import ScrapysItem
import datetime
import uuid
import json
import hashlib
import pymongo
class ${spider_name}Spider(scrapy.Spider):
    name = "${spider_name}"
    def __init__(self):
        mongoclient = pymongo.MongoClient()
        self.spider_collection = mongoclient.spider_content["spider_content"]

    def start_requests(self):
        for i in range(int("${loop_num}")):
            i = (i + int("${start_index}")) * int("${multi_factor}")
            start_url = "${pagelist_get_index}".format(i)
            yield scrapy.Request(start_url,callback=self.parse,dont_filter=True)
    def parse(self, response):
        li_contents = response.xpath("${pagelist_groups_resolving}")
        for li in li_contents:
            detail_url = response.urljoin(li.xpath("${pagelist_url_resolving}").extract()[0].strip())
            yield scrapy.Request(detail_url,callback=self.detail_parse,
                                 dont_filter=True)
    def detail_parse(self,response):
        #tendering = ScrapysItem()



        tendering = {}
        for i in eval("${fields_xpaths}"):
            tendering[i[0]] = response.xpath(i[1]).extract()[0]
        self.spider_collection.insert(tendering)


        #yield tendering

